{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11.NaiveBayes_Homework_ngoquocbao.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPHjnoyxn57x2hDYMdPpP78"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# I. Lý thuyết (10 câu, mỗi câu 0.5 điểm)\n","\n","1. Trong mô hình xác suất Naive Bayes giả định nào được đặt ra và được xem là ngây ngô (naive)? \n"],"metadata":{"id":"12icERfE_8DF"}},{"cell_type":"markdown","source":["2. Giải thích các đại lượng của công thức Bayes sau\n","\n","$$P(y|x) = \\frac{P(y)P(x|y)}{P(x)}$$"],"metadata":{"id":"fkYsSipQZmmq"}},{"cell_type":"markdown","source":["3. Phương pháp MAP có mục tiêu là tối đa hoá hàm mục tiêu là gì? Ưu điểm của MAP so với MLE là gì?"],"metadata":{"id":"NN6kp0ryZq9v"}},{"cell_type":"markdown","source":["Về bản chất MAP cũng là một phương pháp ước lượng tham số của một phân phối xác suất, nhưng khác biết với MLE đó là thay vì tối đa hoá hàm hợp lý thì chúng ta tối đa hoá xác suất hậu nghiệm. Dựa vào công thức Bayes chúng ta có thể phân tích xác suất thành tích của hàm hợp lý với xác suất tiên nghiệm và điều chỉnh niềm tin vào mô hình thông qua xác suất tiên nghiệm. Bài toán tối ưu MAP:\n","\n","![](https://i.imgur.com/HcBiSzO.png)"],"metadata":{"id":"mubzJL5rZth4"}},{"cell_type":"markdown","source":["Ưu điểm của MAP so với MLE là gì?\n","\n","Ở phương pháp MLE chúng ta ước lượng ra phân phối của dữ liệu dựa trên hàm hợp lý. Hàm hợp lý  chỉ được tính trong điều kiện các tham số phân phối đã xác định. Điều đó có nghĩa rằng chúng ta không thể đưa thêm niềm tin của mình vào tham số để tác động lên xác suất. Đây là một hạn chế lớn, đặc biệt là trên những mô hình được hồi qui với kích thước mẫu nhỏ thì qui luật phân phối dựa trên tần suất không còn đáng tin cậy (hãy nhớ về ví dụ tung đồng xu). Khi đó kết quả dự báo sẽ chuẩn xác hơn nếu chúng ta đưa thêm niềm tin vào xác suất.\n","\n","=> Ưu điểm của MAP so với MLE là MAP cho phép ta đưa thêm niềm tin về phân phối tham số vào mô hình."],"metadata":{"id":"kZ1QrImFapYu"}},{"cell_type":"markdown","source":["Một người đi tới công ty làm việc sử dụng 30% thời gian để đi ô tô, 30% thời gian để đi bộ, và 40% đi xe bus. Biết rằng, người đó sẽ bị trễ 10% thời gian khi đi bộ, 3% khi lái xe ô tô và 7% khi đi xe bus. \n","5. Tính xác xuất để người đó đi muộn?"],"metadata":{"id":"ozOsp9IcZQQA"}},{"cell_type":"markdown","source":["![](https://i.imgur.com/57qqb9i.jpg)"],"metadata":{"id":"X8VcOtLcZTnl"}},{"cell_type":"markdown","source":["6. Xác suất người đó đi xe bus nếu anh ta đến muộn là bao nhiêu?"],"metadata":{"id":"wUHnvnm4ZYCT"}},{"cell_type":"markdown","source":["![](https://i.imgur.com/57qqb9i.jpg)"],"metadata":{"id":"vXx6YQO3ZbwS"}},{"cell_type":"markdown","source":["7. Tính xác suất để người đó đến đúng giờ khi đi bộ.\n"],"metadata":{"id":"AhiPWBnsZgVu"}},{"cell_type":"markdown","source":["![](https://i.imgur.com/Rbfukap.jpg)"],"metadata":{"id":"9d6zr5HDZiuc"}},{"cell_type":"markdown","source":["8. Xác suất để một người bị bệnh lao là 0.05%, và xác suất để dương tính là với 99% độ chính xác. Tính xác suất để một người bị bệnh lao nếu người đó khi test bị dương tính. "],"metadata":{"id":"Q3SaUYgPZnDl"}},{"cell_type":"markdown","source":["![](https://i.imgur.com/Rbfukap.jpg)"],"metadata":{"id":"qRs7azT4ZpEh"}},{"cell_type":"markdown","source":["![](https://i.imgur.com/lL0UQQ4.jpg)"],"metadata":{"id":"ztcmkAFtZsRU"}},{"cell_type":"markdown","source":["II. Thực hành (5 câu, mỗi câu 1 điểm)\n","\n","1. Cho bộ dữ liệu về phân loại văn bản gồm các câu theo chủ để như sau:\n","\n","**ẩm thực**:\n","```\n","- phở là một trong những món ăn truyền thống của Hà Nội\n","- hương vị của vịt quay tạo nên một vị phở đậm đà\n","```\n","\n","**thể thao**:\n","```\n","- đội tuyển bóng đá nam đã tạo ra kì tích tại đại hội thể thao Seagame 31.\n","- world cup là mùa giải bóng đá lớn nhất hành tinh.\n","```\n","\n","Hãy xây dựng mô hình `Naive Bayes` với phân phối Multinomial để phân loại các câu văn theo topic mà không sử dụng thư viện scikit-learn."],"metadata":{"id":"1peIJcjVNtmW"}},{"cell_type":"code","source":["import numpy as np \n","\n","class MNNaiveBayes:\n","\n","  def __init__(self, k=0.5):\n","    self.k = k\n","    self.cat0_count = 0\n","    self.cat1_count = 0\n","    self.total_count = self.cat0_count + self.cat1_count\n","    self.cat_0_prior = 0\n","    self.cat_1_prior = 0\n","    self.cat_0_prior, self.cat_1_prior\n","    self.word_probs = []\n","    self.vocab = []\n","\n","  def tokenize(self, document):\n","    \"\"\"\n","    Take in a document and return a list of words\n","    \"\"\"\n","    doc = document.lower()\n","    # remove non-alpha characters\n","    stop_chars = '''0123456789!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    \n","    tokens = \"\"\n","    # iterate through and make each token\n","    for char in doc:\n","      if char not in stop_chars:\n","        tokens += char\n","\n","    return tokens.split() # now a list of tokens\n","  \n","  def count_words(self, X, y):\n","    \"\"\"\n","    X is an array of documents\n","    y is an array of targets, 0 or 1\n","    Output a dictionary of {word: (cat0_count, cat1_count)...}\n","    \"\"\"\n","    counts = {}\n","    # need to figure our this loop, want to iterate over both of them, I see why it was paired before\n","    for document, category in zip(X, y):\n","        for token in self.tokenize(document):\n","          # Initialize a dict entry with 0 counts\n","          if token not in counts:\n","            counts[token] = [0,0]\n","          # Now that it exists, add to the category count for that word\n","          counts[token][category] += 1\n","    return counts\n","\n","  def prior_prob(self, counts):\n","    \n","    # Iterate through counts dict and add up each word count by category\n","    cat0_word_count = cat1_word_count = 0\n","    for word, (cat0_count, cat1_count) in counts.items():\n","        cat0_word_count += cat0_count\n","        cat1_word_count += cat1_count\n","\n","    # save attributes to the class\n","    self.cat0_count = cat0_word_count\n","    self.cat1_count = cat1_word_count\n","    self.total_count = self.cat0_count + self.cat1_count\n","\n","    # Get the prior prob by dividing words in each cat by total words\n","    cat_0_prior = cat0_word_count / self.total_count\n","    cat_1_prior = cat1_word_count / self.total_count\n","    return cat_0_prior, cat_1_prior\n","\n","  def word_probabilities(self, counts):\n","    \"\"\"turn the word_counts into a list of triplets\n","    word, p(w | cat0), and p(w | cat1)\"\"\"\n","    # Here we apply the smoothing term, self.k, so that words that aren't in\n","    # the category don't get calculated as 0\n","    self.vocab = [word for word, (cat0, cat1) in counts.items()]\n","    return [(word,\n","    (cat0 + self.k) / (self.cat0_count + 2 * self.k),\n","    (cat1 + self.k) / (self.cat1_count + 2 * self.k))\n","    for word, (cat0, cat1) in counts.items()]\n","\n","  def fit(self, X, y):\n","    # Take all these functions and establish probabilities of input\n","    counts = self.count_words(X, y)\n","    self.cat_0_prior, self.cat_1_prior = self.prior_prob(counts)\n","    self.word_probs = self.word_probabilities(counts)\n","\n","  def predict(self, test_corpus):\n","    # Split the text into tokens,\n","    # For each category: calculate the probability of each word in that cat\n","    # find the product of all of them and the prior prob of that cat\n","    y_pred = []\n","    for document in test_corpus:\n","      # Every document get their own prediction probability\n","      log_prob_cat0 = log_prob_cat1 = 0.0\n","      tokens = self.tokenize(document)\n","        # Iterate through the training vocabulary and add any log probs that match\n","        # if no match don't do anything. We just need a score for each category/doc\n","      for word, prob_cat0, prob_cat1 in self.word_probs:\n","        if word in tokens:\n","          # Because of 'overflow' best to add the log probs together and exp\n","          log_prob_cat0 += np.log(prob_cat0)\n","          log_prob_cat1 += np.log(prob_cat1)\n","        # get each of the category predictions including the prior\n","      cat_0_pred = self.cat_0_prior * np.exp(log_prob_cat0)\n","      cat_1_pred = self.cat_1_prior * np.exp(log_prob_cat1)\n","      if cat_0_pred >= cat_1_pred:\n","        y_pred.append(0)\n","      else:\n","        y_pred.append(1)\n","    return y_pred\n","      "],"metadata":{"id":"fZIgreAPTzFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = ['phở là một trong những món ăn truyền thống của Hà Nội.', 'hương vị của vịt quay tạo nên một vị phở đậm đà.', 'đội tuyển bóng đá nam đã tạo ra kì tích tại đại hội thể thao Seagame 31.', 'world cup là mùa giải bóng đá lớn nhất hành tinh.']\n","#y = ['ẩm thực', 'ẩm thực', 'thể thao', 'thể thao']\n","y = [0,0,1,1]"],"metadata":{"id":"X_VdbXP-RNg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb = MNNaiveBayes()\n","nb.fit(X, y)"],"metadata":{"id":"bHFFIT5PUG9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2. Sử dụng mô hình vừa huấn luyện, hãy dự báo nhãn cho các văn bản sau:\n","\n","```\n","các món ăn đường phố là một văn hóa truyền thống của người Việt.\n","```\n","và \n","\n","```\n","Quang Hải đã được mua lại bởi một đội bóng truyền thống tại Pháp."],"metadata":{"id":"BjtLgaGjXbyW"}},{"cell_type":"code","source":["X_test = ['các món ăn đường phố là một văn hóa truyền thống của người Việt.', 'Quang Hải đã được mua lại bởi một đội bóng truyền thống tại Pháp.']\n","print(nb.predict(X_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JisDEiYXcc7","executionInfo":{"status":"ok","timestamp":1660766072475,"user_tz":-180,"elapsed":650,"user":{"displayName":"Quoc Bao Ngo","userId":"17034167578407563163"}},"outputId":"55bd24b5-d24e-412c-858a-b971e83619aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1]\n"]}]},{"cell_type":"markdown","source":["3. Xây dựng mô hình `NaiveBayes Classifier` với phân phối Gaussian để phân loại các loài hoa từ bộ dữ liệu iris mà không cần sử dụng scikit-learn. Để load bộ dữ liệu iris có thể sử dụng sklearn như sau:"],"metadata":{"id":"3IEMzI97X_g3"}},{"cell_type":"code","source":["from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target"],"metadata":{"id":"69cgsXC1YAuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GaussianNaiveBayes:\n","     def fit(self, X, y):\n","         n_samples, n_features = X.shape\n","         self._classes = np.unique(y)\n","         n_classes = len(self._classes)\n","         self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n","         self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n","         self._priors =  np.zeros(n_classes, dtype=np.float64)\n","\n","         # calculating the mean, variance and prior P(H) for each class\n","         for i, c in enumerate(self._classes):\n","             X_for_class_c = X[y==c]\n","             self._mean[i, :] = X_for_class_c.mean(axis=0)\n","             self._var[i, :] = X_for_class_c.var(axis=0)\n","             self._priors[i] = X_for_class_c.shape[0] / float(n_samples)\n","\n","     def _calculate_likelihood(self, class_idx, x):\n","         mean = self._mean[class_idx]\n","         var = self._var[class_idx]\n","         num = np.exp(- (x-mean)**2 / (2 * var))\n","         denom = np.sqrt(2 * np.pi * var)\n","         return num / denom \n","\n","     def predict(self, X):\n","         y_pred = [self._classify_sample(x) for x in X]\n","         return np.array(y_pred)\n","\n","     def _classify_sample(self, x):\n","         posteriors = []\n","         # calculating posterior probability for each class\n","         for i, c in enumerate(self._classes):\n","             prior = np.log(self._priors[i])\n","             posterior = np.sum(np.log(self._calculate_likelihood(i, x)))\n","             posterior = prior + posterior\n","             posteriors.append(posterior)\n","         # return the class with highest posterior probability\n","         return self._classes[np.argmax(posteriors)]"],"metadata":{"id":"S1u81em7b3Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nb = GaussianNaiveBayes()\n","nb.fit(X, y)"],"metadata":{"id":"bKN4tuPUcVEM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","4. Dự báo đối với hai quan sát mới có giá trị \n","`['sepal length (cm)',\n","  'sepal width (cm)',\n","  'petal length (cm)',\n","  'petal width (cm)']` lần lượt như sau:\n","\n","```\n","[6.4, 3.1, 5.5, 1.8]\n","và  \n","[4.9, 3. , 1.4, 0.2],\n","```\n"],"metadata":{"id":"NVJJdinNcajy"}},{"cell_type":"code","source":["print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sQSwnqwetMW","executionInfo":{"status":"ok","timestamp":1660856663971,"user_tz":-180,"elapsed":330,"user":{"displayName":"Quoc Bao Ngo","userId":"17034167578407563163"}},"outputId":"cd6ba903-4033-46a5-92dd-4112db8875cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n"," 2 2]\n"]}]},{"cell_type":"code","source":["print(nb.predict([[6.4, 3.1, 5.5, 1.8], [4.9, 3. , 1.4, 0.2]]))\n","#print(nb.predict([4.9, 3. , 1.4, 0.2]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKfb2ffwcatq","executionInfo":{"status":"ok","timestamp":1660856666681,"user_tz":-180,"elapsed":311,"user":{"displayName":"Quoc Bao Ngo","userId":"17034167578407563163"}},"outputId":"80d3fa2d-4c96-411b-f2f9-aa413bae0621"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 0]\n"]}]},{"cell_type":"markdown","source":["5. Thực hành xây dựng lại mô hình `NaiveBayes` theo package `scikit-learn` để phân loại các loài hoa trong bộ dữ liệu iris."],"metadata":{"id":"5TTSaHIixZHI"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import classification_report\n","from sklearn.naive_bayes import GaussianNB\n","\n","X, y = load_iris(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n","\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","\n","y_pred = gnb.predict(X_test)\n"],"metadata":{"id":"0ShMUXucxZgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dự báo đối với hai quan sát mới có giá trị ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] lần lượt như sau:\n","#[6.4, 3.1, 5.5, 1.8]\n","#và  \n","#[4.9, 3. , 1.4, 0.2], \n","\n","\n","y_pred2 = gnb.predict([[6.4, 3.1, 5.5, 1.8], [4.9, 3. , 1.4, 0.2]])\n","print(y_pred2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTumJ34lxtKg","executionInfo":{"status":"ok","timestamp":1660856776242,"user_tz":-180,"elapsed":14,"user":{"displayName":"Quoc Bao Ngo","userId":"17034167578407563163"}},"outputId":"42cf48bc-250b-483c-85f4-b949cb4d58d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2 0]\n"]}]}]}